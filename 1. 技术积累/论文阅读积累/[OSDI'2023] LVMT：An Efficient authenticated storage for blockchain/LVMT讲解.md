大家好，我今天要分享的是OSDI2023的一篇会议论文LVMT：An Efficient authenticated storage for blockchain，该论文主要围绕区块链中的认证存储结构提出了一种新的方案降低了这一环节的overhead

首先介绍一下该论文提出的背景：从区块链性能的发展历程看，最初的比特币和以太坊只能达到低于每秒30条交易的吞吐量，而用于转账交易的visa可以做到每秒处理20000条交易。其主要的瓶颈在于账本达成共识阶段的时间开销，近年来通过共识协议上的创新，区块链已经可以达到与visa同一量级的吞吐量，为进一步提高吞吐量，目前的瓶颈在于执行交易时在存储层的读写操作，交易执行时间中的80%都消耗在了存储层,  其中的原因如下：

先看区块链的存储结构，以太坊在比特币基础上提出了基于账户的模型，账户的概念使得区块链系统能够引入智能合约，实现多样的应用，在以太坊中包含了一棵MPT结构的账户状态树，该树以key-value pair的形式保存了账户与账户信息，key对应的是账户，value对应的是账户余额、交易次数等账户信息，MPT中的每个叶子节点都是一个账户，这棵状态树是动态变化的 ，这颗MPT树就是区块链系统中的authenticated data structure,以太坊及之后提出的公链都是基于账户的模型，本篇论文也是对基于账户的模型的存储层结构提出的一种更优的方案（1min）

从区块链执行交易的过程观察authenticated data structure，首先经过排序的交易放入虚拟机被执行，在执行交易过程中会对账户信息进行读写，如果账户信息在cache中不存在，则需要访问数据库，由于数据是按authenticated data structure这一结构进行组织的，在访问所需要的数据时，需要访问authenticated data structure，对这一数据结构的访问导致了磁盘的读写次数会被放大，而该结构的作用是为区块链系统提供Proof功能。

Proof功能基本逻辑如下，区块链中的节点分为全节点和轻节点，全节点保存所有账本信息，轻节点只保存区块中的block header，而并不保存交易列表，当轻节点需要查询一个账户的余额时，需要向全节点发送询问，由全节点提供该账户的相关信息，由于在公链中全节点可能是带有恶意的，故要求全节点回复账户信息的同时提供相应的proof，轻节点可以通过全节点提供的proof验证全节点回复的账户信息的真伪，虽然proof也是由全节点提供的，但是哈希函数的collision resistance性质保证了proof无法伪造。

那提供Proof功能的authenticated data structure为什么会导致磁盘读写次数的放大呢，我们以MPT解释其中的过程，此前的区块链系统中大多使用了MPT这一Merkle tree的变种作为Authenticated data structure，MPT结构相对复杂，但其导致磁盘读写次数放大的原理与Merkle tree相同，这里以Merkle tree为例解释该原因，该结构自底向上由叶子节点通过哈希函数计算得到父节点直至根哈希值，每个叶子节点存储了一个key,value pair, 这一结构中当key对应的value发生变化时，需要沿着从叶子节点到根节点的路径重新计算哈希值，即更新Commitment需要O（logn）的时间复杂度，也就是说对一叶子节点进行写操作时，需要对树的中间节点也进行写操作，读操作也是类似的原理，需要访问树的中间节点。这是导致读写操作会在存储层发生放大的根本原因。

同时我们借助该图解释一下全节点向轻节点提供proof的原理，轻节点只拥有状态树中的commitement，即root hash这一个值，当轻节点询问棕色路径上叶子节点的账户信息时，全节点需要提供途中用红色框标注的三个节点哈希值作为proof。故proof generation的时间复杂度也是O（logn）

正是这树的结构使得读写操作需要访问中间节点，这导致了读写次数的放大，故本文采用了AMT结构取代MPT，这是因为AMT可以不需要中间节点直接更新commitment,其时间复杂度为O（1），其原理可以概括为引入了椭圆曲线加密算法, 但AMT在提供proof时仍然需要中间节点信息，时间复杂度同样是O（logn）。AMT能够在常数时间下更新commitment的性质使全节点在执行交易过程中，更新叶子节点信息时不需要访问中间节点，故而解决了磁盘读写放大的问题。（4 min）

但是引入AMT也存在着以下三个 挑战：第一，AMT虽然能以常数时间更新commitment，但是这一更新操作包含的密码学计算代价大，计算时间较久。第二，AMT并不像MPT具备可扩展能力，并且其最大容量扩大时，在定义结构时需要预先计算的参数量也随之扩大。第三，AMT提供proof的时间复杂度与MPT相同依然是O（logn）

第一个挑战具体而言 在于以常数时间更新commitment时其中包含的椭圆曲线乘法操作开销较大，这里ai即key-value pair中的value，ai‘是交易发生后value的值，如果这个差值是1，那更新commitment时开销最大的乘法操作便被避开了。于是作者提出了value的版本号的概念，即把AMT中存储的key value pair改为存储key version pair，而value则存储在一系列的Merkle tree中，每当某一个key的value发生改变时，将AMT中对应key的version + 1,再将新的value与新的版本号等信息构成的元组存储在Merkle tree中，这样使得AMT在更新commitment时版本号的差值总是1，即可避开椭圆曲线乘法。

第二个挑战是AMT是不可扩展的，AMT需要在初始化阶段确定key的位数k，同时需要预先计算2^k个参数用于更新commitment，通常比特币系统中的key是256bit，尽管k=256可以提供2的256次方个账户，但是也需要2的256次方个预计算参数，即使k=32，预计算参数也已经达到了256GB，故采用k=256是不现实的。                          于是作者提出了多级AMT的结构，每一个AMT采取k=16，将预计算参数的指数增长转化为了线性增长，这与操作系统中多级存储类似，并且多级AMT结构在账户数量增长时会自动构建新的子树，实现了动态扩展。具体存储时，如果有一个key的前k位与AMT中的索引匹配，则在该索引对应的槽位中插入该key对应value的版本号，并将key与version的对应信息存在一个额外的Map结构VM中。为了减少多级AMT的深度，每个AMT的一个索引将对应6个槽位，只有当槽位满额时，才会构建AMT子树。

第三个挑战在于全节点在提供proof 时，仍然需要维护AMT叶子节点到AMT的根哈希之间的辅助信息，并和MPT相同需要O（logn）的时间复杂度。                   作者对该问题采用了Proof Sharding方式，Proof Sharding方法是比较成熟的一种减少分布式系统中单个节点负担的方法，在其他论文中已经有过较多应用，比如每个full node只需要存储一部分AMT子树的中间节点，比如只存储前4位key值相同的账户所在的子树，最终由多个full node协作来产生proof，Proof sharding技术也为网络中专门提供生成Proof服务的 Service Provider降低了overhead。

（4min）

作者通过模拟的方式进行了实验，作者构造了不同规模和不同类型的执行任务，并对比了执行任务在不同认证存储结构上的表现，LVMT-r是只保存commitment而不提供Proof的实现，不提供Proof是指该实现中AMT不保留其中间节点，LVMT#是提供proof的实现，#表示一个数字，代表着分片数量，如LVMT64是将AMT树划分为64个分片，实验以MPT、RAIN、LMPTs这三个应用较广泛，具有代表性的authenticated Data Structure作为baseline。

实验的性能指标主要包括存储结构的吞吐量，读写操作次数放大率，存储时间在执行交易总时间中的占比以及整个系统的交易吞吐量。



图中的标注，带有LVMT的是不同的实现方式，RAIN是MPT的变种，是公链RainBlock所使用的结构

首先是在认证存储结构的吞吐量方面，LVMT-r相比MPT至少提升了353%，相比RAIN至少提升了80%，这是因为LVMT的设计减少了在读写次数的放大，当需要维持提供proof的辅助信息时，LVMT64相比LVMT-r吞吐量降低了20%，LVMT16相比LVMT-r降低了40%，这是因为维护辅助信息增加了部分对中间节点的访问，但其吞吐量仍然是要高于MPT和RAIN，但实验结果中可以看到LVMT-1的吞吐量非常低，这是因为每个节点要维护整个多级AMT的辅助信息会导致时间开销巨大，说明proof sharding在LVMT中是必要的，此外当账本规模达到100million的级别时，所有结构的吞吐量都显著降低了，这是因为账本规模的扩大导致状态树也随之增长，维护状态树的开销也会增长。

接下来看论文重点解决的读写放大问题，读操作的实验结果中，LVMT-r在不同规模账本下都做到了对读操作放大的限制，LVMT64和LVMT16有所一定程度的放大，这是因为需要维护辅助信息，读写次数放大来源于对辅助信息的访问，但是相比MPT也有所优化，RAIN由于自身附带的Cache机制，读写次数放大很少。

在交易执行时间占比中，LVMT-r和LVMT64在Authenticated Structure中所消耗的时间降到了与直接访问后端数据库接近的程度，而MPT和RAIN在认证存储过程中的开销是比较大的。这也是来源于磁盘读写次数的减少，并说明LVMT确实减少了执行交易过程中的主要时间开销。

在整个系统的交易吞吐量表现上，LVMT-r是RAIN的1.7倍，是MPT的2.7倍，这也得益于读写放大操作的减少

最后是对本片论文的反思，

第一：LVMT虽然做到以常数时间更新commitment，但是维护提供proof的辅助信息还是会和MPT结构一样导致类似的读写放大，尽管放大程度有所减少，因为在实际网络不可能使用LVMT-r这样不提供proof的结构， 或许在proof sharding技术上的提升可以进一步提高LVMT的性能

第二：既然一定程度上的读写次数放大难以避免，或许可以优化cache机制，减少对数据库的访问。





问题：

我们现在不是去分析这篇论文，而是去学习如何写？

分析实验结果是要把为什么实验结果变好，说出来

实验部分要讲实验是如何implemented，软硬件，跟谁对比，实验结果中的指标是什么要进行讲解

最后要加上对这篇论文的思考，即有什么可以改进的地方

background和introduction的区别，background讲的是这个东西是什么，what,why,how,而introduction就是讲为什么要搞这个东西，困难点在哪里，解决方案是什么？